{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98723,"databundleVersionId":11781620,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport optuna\nimport random\nimport time\nimport datetime\nimport re, string, html\n# import logging\n# logging.basicConfig(level=logging.INFO)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim import AdamW\n\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.metrics import (accuracy_score, precision_recall_curve, f1_score,\n    roc_curve, confusion_matrix, classification_report, ConfusionMatrixDisplay, auc)\n\nfrom sklearn.manifold import TSNE\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set a random seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')\n\n# If there's a GPU available...\nif torch.cuda.is_available():\n\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \n# Load datasets\ntrain_ds = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-3/train_dataset.csv')\nval_ds = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-3/val_dataset.csv')\ntest_ds = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-3/test_dataset.csv')\n\n# Check data is imported correctly\nprint(train_ds.head)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preprocessing (from HW1) but with Bert Tokenizer\ntk = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocessing(text):\n    # Decode HTML entities\n    text = html.unescape(text)\n    # Make text lowercase\n    text = text.lower()\n    # Change emails to xxx@email.com\n    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', 'xxx@email.com', text)\n    # Remove Twitter usernames\n    text = re.sub(r'@\\w+', '', text)\n    # Change URLs to xxx.link.com\n    text = re.sub(r'https?://\\S+|www\\.\\S+', 'httpxxx', text)\n     # Remove punctuation\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Remove non-alphanumeric characters and emojis\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Replace repeating characters (3 or more) with just 2 letters\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    # Tokenize with TweetTokenizer\n    tokens = tk.tokenize(text)\n    # Define stopwords to filter out of tokens list\n    # stop_words = {'the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'in', 'so',\n    #              'omg', 'dude', 'lol', 'my', 'for', 'on', 'you', 'it', 'me'}\n    # Lemmatize\n    tokens = [ lemmatizer.lemmatize(word) \n               for word in tokens]\n                   #if word not in stop_words and word.isalpha()]\n    return ' '.join(tokens)\n\n# Text\ntrain_ds['preprocessed_text'] = train_ds['Text'].apply(preprocessing)\nval_ds['preprocessed_text'] = val_ds['Text'].apply(preprocessing)\ntest_ds['preprocessed_text'] = test_ds['Text'].apply(preprocessing)\n\n# Check data is valid and correctly loaded\nprint(train_ds.head)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenization for BERT\ntrain_texts = train_ds['preprocessed_text'].values\nval_texts = val_ds['preprocessed_text'].values\ntest_texts = test_ds['preprocessed_text'].values\n\ntrain_labels = train_ds['Label'].values\nval_labels = val_ds['Label'].values\n\ndef get_max_len(texts):\n    return max([len(tokenizer.encode(text, add_special_tokens=True)) for text in texts])\n\n#print(\"Adding +10 to the sentence length for safety.\")\nmax_len_train = get_max_len(train_texts) \nprint(\"Maximum sentence length in train set: \", max_len_train)\nmax_len_val = get_max_len(val_texts) \nprint(\"Maximum sentence length in val set: \", max_len_val)\nmax_len_test = get_max_len(test_texts) \nprint(\"Maximum sentence length in test set: \", max_len_test)\n\n# Maximum stentence length in train set:  54\n# Maximum stentence length in val set:  49\n# Maximum stentence length in test set:  52\n# So we will manually assign max_len = 64\nMAX_LEN = 64\ndef bert_encode(texts, tokenizer, MAX_LEN):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    return (\n        torch.cat(input_ids, dim=0),\n        torch.cat(attention_masks, dim=0)\n    )\n\n# Encode all sets\ninput_ids_train, attention_masks_train = bert_encode(train_texts, tokenizer, MAX_LEN)\ninput_ids_val, attention_masks_val = bert_encode(val_texts, tokenizer, MAX_LEN)\ninput_ids_test, attention_masks_test = bert_encode(test_texts, tokenizer, MAX_LEN)\n\n# Convert labels to tensors\nlabels_train = torch.tensor(train_labels)\nlabels_val = torch.tensor(val_labels)\n\n# Print shape info\nprint(\"Train input_ids shape:\", input_ids_train.shape)\nprint(\"Validation input_ids shape:\", input_ids_val.shape)\nprint(\"Test input_ids shape:\", input_ids_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create TensorDatasets\ntrain_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\nval_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\ntest_dataset = TensorDataset(input_ids_test, attention_masks_test)\n\n# Define batch size \nBATCH_SIZE = 32\n\n# Create DataLoaders\ntrain_dataloader = DataLoader(\n    train_dataset,\n    sampler = RandomSampler(train_dataset),\n    batch_size = BATCH_SIZE\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    sampler = SequentialSampler(val_dataset),\n    batch_size = BATCH_SIZE\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    sampler = SequentialSampler(test_dataset),\n    batch_size = BATCH_SIZE\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()\n\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer & Learning Rate Scheduler\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, #3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\nepochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs].\ntotal_steps = len(train_dataloader) * epochs\n\n\n# later  experiment:\nnum_warmup_steps=int(0.1 * total_steps)\n\n# Create the learning rate scheduler to dynamically adjust the learning rate.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=num_warmup_steps, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef format_time(elapsed):\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n    # We'll store a number of quantities such as training and validation loss,","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# validation accuracy, and timings.\ntraining_stats = []\ntrain_losses = []\nval_losses = []\nval_accuracies = []\nval_f1s = []\nmodel_outputs = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# Training Loop\nfor epoch_i in range(epochs):\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n    # Put the model into training mode.\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            \n        # Unpack this training batch from our dataloader.\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        #Clear any previously calculated gradients before performing a backward pass\n        model.zero_grad()\n\n        result = model(b_input_ids,\n                   token_type_ids=None,\n                   attention_mask=b_input_mask,\n                   labels=b_labels,\n                   return_dict=True)\n\n        loss = result.loss\n        logits = result.logits\n            \n        # Accumulate the training loss over all of the batches\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n            \n        # Update parameters and take a step using the computed gradient.            \n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_losses.append(avg_train_loss)\n\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(training_time))\n\n    # ========================================\n    #               Validation\n    # ========================================\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n\n    val_preds = []\n    val_labels = []\n    model_outputs_epoch = []\n    # Evaluate data for one epoch\n    for batch in val_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            result = model(b_input_ids,\n                          token_type_ids=None,\n                          attention_mask=b_input_mask,\n                          labels=b_labels,\n                          return_dict=True)\n\n        # Get the loss and \"logits\" output by the model. The \"logits\" are the\n        # output values prior to applying an activation function like the\n        # softmax.\n        loss = result.loss\n\n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = result.logits.detach().cpu().numpy()\n        probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n        model_outputs_epoch.extend(probs[:, 1]) # for ROC Curve\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n        # For Classification Report\n        batch_preds = np.argmax(logits, axis = 1).flatten()\n        val_preds.extend(batch_preds)\n        val_labels.extend(label_ids)\n    \n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    # For the plots\n    val_f1 = f1_score(val_labels, val_preds, average='macro')\n    val_accuracies.append(avg_val_accuracy)\n    val_losses.append(avg_val_loss)\n    val_f1s.append(val_f1)\n    model_outputs = model_outputs_epoch  # update most recent for ROC/PR curve\n\n    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    print(\"\\nClassification Report:\")\n    print(classification_report(val_labels, val_preds, target_names=['Negative', 'Positive']))\n\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Valid. F1': val_f1,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert stats to DataFrame\ndf_stats = pd.DataFrame(training_stats).set_index('epoch')\n\n# Seaborn styling\nsns.set(style='darkgrid')\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_losses, 'b-o', label=\"Training Loss\")\nplt.plot(val_losses, 'g-o', label=\"Validation Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks(range(epochs))\nplt.savefig(\"train_val_loss_over_epochs.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Val Accuracy over Epochs\nplt.plot(val_accuracies, 'r-o', label=\"Validation Accuracy\")\nplt.title(\"Validation Accuracy Over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.legend()\nplt.xticks(range(epochs))\nplt.savefig(\"accuracy_over_epochs.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# F1 score plot\nplt.plot(val_f1s, color='purple', label=\"Validation F1 Score\")\nplt.title(\"Validation F1 Score Over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1 Score\")\nplt.grid(True)\nplt.legend()\nplt.xticks(range(len(val_losses)))\nplt.savefig(\"f1_score_over_epochs.png\")\nplt.show()\n\n# Confusion Matrix (Final Evaluation)\ncm = confusion_matrix(val_labels, val_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"confusion_matrix.png\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(val_labels, model_outputs)  # model_outputs = raw sigmoid outputs\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"roc_curve.png\")\nplt.show()\n\n# Precision - Recall Curve\nprecision, recall, _ = precision_recall_curve(val_labels, model_outputs)\n\nplt.plot(recall, precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precisionâ€“Recall Curve\")\nplt.grid(True)\nplt.savefig(\"precision_recall_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction on test set\nprint('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables\npredictions  = []\n\n# Predict\nfor batch in test_dataloader:\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask = [t.to(device) for t in batch]\n\n  # Telling the model not to compute or store gradients, saving memory and\n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions.\n      result = model(b_input_ids,\n                     token_type_ids=None,\n                     attention_mask=b_input_mask,\n                     return_dict=True)\n\n  # Move logits and labels to CPU\n  logits = result.logits.detach().cpu().numpy()\n\n  # Store predictions and true labels\n  predictions.append(logits)\n\nprint('    DONE.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine the results across all batches.\nflat_predictions = np.concatenate(predictions, axis=0)\n\n# For each sample, pick the label (0 or 1) with the higher score.\npredicted_labels = np.argmax(flat_predictions, axis=1).flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'Id': test_ds['ID'],  \n    'Label': predicted_labels\n})\n\n# Save submission to .csv\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file created succesfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}